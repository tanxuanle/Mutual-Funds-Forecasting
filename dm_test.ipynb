{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "from typing import Sequence, Callable, List, Tuple\n",
    "from math import lgamma, fabs, isnan, nan, exp, log, log1p, sqrt\n",
    "\n",
    "class InvalidParameterException(Exception):\n",
    "    def __init__(self, message: str):\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "class ZeroVarianceException(ArithmeticError):\n",
    "    def __init__(self, message: str):\n",
    "        super().__init__(message)\n",
    "\n",
    "\n",
    "def autocovariance(X: Sequence[float], k: int, mean: float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the k-lagged autocovariance for the input iterable.\n",
    "    \"\"\"\n",
    "    return sum((a - mean) * (b - mean) for a, b in zip(islice(X, k, None), X)) / len(X)\n",
    "\n",
    "\n",
    "def log_beta(a: float, b: float) -> float:\n",
    "    \"\"\"\n",
    "    Returns the natural logarithm of the beta function computed on\n",
    "    arguments `a` and `b`.\n",
    "    \"\"\"\n",
    "    return lgamma(a) + lgamma(b) - lgamma(a + b)\n",
    "\n",
    "def evaluate_continuous_fraction(\n",
    "    fa: Callable[[int, float], float],\n",
    "    fb: Callable[[int, float], float],\n",
    "    x: float,\n",
    "    *,\n",
    "    epsilon: float = 1e-10,\n",
    "    maxiter: int = 10000,\n",
    "    small: float = 1e-50\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate a continuous fraction.\n",
    "    \"\"\"\n",
    "    h_prev = fa(0, x)\n",
    "    if fabs(h_prev < small):\n",
    "        h_prev = small\n",
    "\n",
    "    n: int = 1\n",
    "    d_prev: float = 0.0\n",
    "    c_prev: float = h_prev\n",
    "    hn: float = h_prev\n",
    "\n",
    "    while n < maxiter:\n",
    "        a = fa(n, x)\n",
    "        b = fb(n, x)\n",
    "\n",
    "        dn = a + b * d_prev\n",
    "        if fabs(dn) < small:\n",
    "            dn = small\n",
    "\n",
    "        cn = a + b / c_prev\n",
    "        if fabs(cn) < small:\n",
    "            cn = small\n",
    "\n",
    "        dn = 1 / dn\n",
    "        delta_n = cn * dn\n",
    "        hn = h_prev * delta_n\n",
    "\n",
    "        if fabs(delta_n - 1.0) < epsilon:\n",
    "            break\n",
    "\n",
    "        d_prev = dn\n",
    "        c_prev = cn\n",
    "        h_prev = hn\n",
    "\n",
    "        n += 1\n",
    "\n",
    "    return hn\n",
    "\n",
    "\n",
    "def regularized_incomplete_beta(\n",
    "    x: float, a: float, b: float, *, epsilon: float = 1e-10, maxiter: int = 10000\n",
    ") -> float:\n",
    "    if isnan(x) or isnan(a) or isnan(b) or x < 0 or x > 1 or a <= 0 or b <= 0:\n",
    "        return nan\n",
    "\n",
    "    if x > (a + 1) / (2 + b + a) and 1 - x <= (b + 1) / (2 + b + a):\n",
    "        return 1 - regularized_incomplete_beta(\n",
    "            1 - x, b, a, epsilon=epsilon, maxiter=maxiter\n",
    "        )\n",
    "\n",
    "    def fa(n: int, x: float) -> float:\n",
    "        return 1.0\n",
    "\n",
    "    def fb(n: int, x: float) -> float:\n",
    "        if n % 2 == 0:\n",
    "            m = n / 2.0\n",
    "            return (m * (b - m) * x) / ((a + (2 * m) - 1) * (a + (2 * m)))\n",
    "\n",
    "        m = (n - 1.0) / 2.0\n",
    "        return -((a + m) * (a + b + m) * x) / ((a + (2 * m)) * (a + (2 * m) + 1.0))\n",
    "\n",
    "    return exp(\n",
    "        a * log(x) + b * log1p(-x) - log(a) - log_beta(a, b)\n",
    "    ) / evaluate_continuous_fraction(fa, fb, x, epsilon=epsilon, maxiter=maxiter)\n",
    "\n",
    "def dm_test(\n",
    "    V: Sequence[float],\n",
    "    P1: Sequence[float],\n",
    "    P2: Sequence[float],\n",
    "    *,\n",
    "    loss: Callable[[float, float], float] = lambda u, v: (u - v) ** 2,\n",
    "    h: int = 1,\n",
    "    one_sided: bool = False,\n",
    "    harvey_correction: bool = True\n",
    ") -> Tuple[float, float]:\n",
    "    r\"\"\"\n",
    "    Performs the Diebold-Mariano test. The null hypothesis is that the two forecasts (`P1`, `P2`) have the same accuracy.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    V: Sequence[float]\n",
    "        The actual timeseries.\n",
    "\n",
    "    P1: Sequence[float]\n",
    "        First prediction series.\n",
    "\n",
    "    P2: Sequence[float]\n",
    "        Second prediction series.\n",
    "\n",
    "    loss: Callable[[float, float], float]\n",
    "        Loss function. At each time step of the series, each prediction is charged a loss, \n",
    "        computed as per this function. The Diebold-Mariano test is agnostic with respect to \n",
    "        the loss function, and this implementation supports arbitrarily specified (for example asymmetric) \n",
    "        functions. The two arguments are, *in this order*, the actual value and the predicted value. \n",
    "        Default is squared error (i.e. `lambda u, v: (u - v) ** 2`)\n",
    "\n",
    "    h: int\n",
    "        The forecast horizon. Default is 1.\n",
    "\n",
    "    one_sided: bool\n",
    "        If set to true, returns the p-value for a one-sided test instead of a two-sided test. Default is false.\n",
    "\n",
    "    harvey_correcetion: bool\n",
    "        If set to true, uses a modified test statistics as per Harvey, Leybourne and Newbold (1997).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of two values. The first is the test statistic, the second is the p-value.\n",
    "    \"\"\"\n",
    "    if not (len(V) == len(P1) == len(P2)):\n",
    "        raise InvalidParameterException(\n",
    "            \"Actual timeseries and prediction series must have the same length.\"\n",
    "        )\n",
    "\n",
    "    if h <= 0:\n",
    "        raise InvalidParameterException(\n",
    "            \"Invalid parameter for horizon length. Must be a positive integer.\"\n",
    "        )\n",
    "\n",
    "    V = V.values.tolist()\n",
    "    P1 = P1.values.tolist()\n",
    "    P2 = P2.values.tolist()\n",
    "\n",
    "    n = len(P1)\n",
    "    mean = 0.0\n",
    "    loss1 = 0.0\n",
    "    loss2 = 0.0\n",
    "    D: List[float] = []\n",
    "\n",
    "    '''\n",
    "    l1 += loss(v, p1)\n",
    "    l2 += loss(v, p2)\n",
    "    mean += l1 - l2\n",
    "    '''\n",
    "    for i in range(0,n):\n",
    "        l1 = loss(V[i][0], P1[i][0])\n",
    "        l2 = loss(V[i][0], P2[i][0])\n",
    "        D.append(l1 - l2)\n",
    "        mean += l1 - l2\n",
    "        loss1 += l1\n",
    "        loss2 += l2\n",
    "\n",
    "    mean /= n\n",
    "    \n",
    "    '''\n",
    "    for v, p1, p2 in zip(V, P1, P2):\n",
    "        l1 = loss(v, p1)\n",
    "        l2 = loss(v, p2)\n",
    "        D.append(l1 - l2)\n",
    "        mean += l1 - l2\n",
    "        loss1 += l1\n",
    "        loss2 += l2\n",
    "\n",
    "    mean /= n\n",
    "    '''\n",
    "    \n",
    "    V_d = 0.0\n",
    "    for i in range(h):\n",
    "        V_d += autocovariance(D, i, mean)\n",
    "        if i == 0:\n",
    "            V_d /= 2\n",
    "\n",
    "    V_d = 2 * V_d / n\n",
    "\n",
    "    if V_d == 0:\n",
    "        raise ZeroVarianceException(\n",
    "            \"Variance of the DM statistic is zero. Maybe the prediction series are identical?\"\n",
    "        )\n",
    "\n",
    "    if harvey_correction:\n",
    "        harvey_adj = sqrt((n + 1 - 2 * h + h * (h - 1) / n) / n)\n",
    "        dmstat = harvey_adj / sqrt(V_d) * mean\n",
    "    else:\n",
    "        dmstat = mean / sqrt(V_d)\n",
    "\n",
    "    pvalue = regularized_incomplete_beta(\n",
    "        (n - 1) / ((n - 1) + dmstat ** 2), 0.5 * (n - 1), 0.5\n",
    "    )\n",
    "\n",
    "    if one_sided:\n",
    "        # please change dmstat < 0 or dmstat > 0; also change the num_of_NE.csv in 07.ipynb \n",
    "        if dmstat > 0:\n",
    "            pvalue = pvalue \n",
    "        else:\n",
    "            pvalue = 1 \n",
    "\n",
    "    return dmstat, pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_diebold_mariano(dataset, target_var, model_1, model_2):\n",
    "    pred = dataset.copy()\n",
    "    if len(pred[[model_1]].dropna()) < len(pred[[model_2]].dropna()):\n",
    "        model_1_val = pred[[model_1]].dropna()\n",
    "        window_start = model_1_val.index[0]\n",
    "        window_end = model_1_val.index[-1]\n",
    "        model_2_val = pred[[model_2]].loc[window_start:window_end].copy()\n",
    "        y_val = pred[[target_var]].loc[window_start:window_end].copy()\n",
    "\n",
    "\n",
    "        return model_1_val, model_2_val, y_val\n",
    "    else:\n",
    "        model_2_val = pred[[model_2]].dropna()\n",
    "        window_start = model_2_val.index[0]\n",
    "        window_end = model_2_val.index[-1]\n",
    "        model_1_val = pred[[model_1]].loc[window_start:window_end].copy()\n",
    "        y_val = pred[[target_var]].loc[window_start:window_end].copy()\n",
    "\n",
    "        return model_1_val, model_2_val, y_val\n",
    "\n",
    "def evaluate_pvalue(pvalue):\n",
    "    if pvalue < 0.05:\n",
    "    #non-equivalent, i.e. we reject the null hypothesis that both models have equal predictive capability\n",
    "    #non-equivalence in RED\n",
    "        pvalue = -1\n",
    "    else:\n",
    "    #pvalue > 0.05\n",
    "    #equivalent, i.e. we accept the null hypothesis that both models have equal predictive capability\n",
    "    #not enough evidence to show that one model predictive better than the other\n",
    "    #equivalence in BLACK\n",
    "        pvalue = 1\n",
    "    return pvalue\n",
    "\n",
    "def generate_diebold_mariano(dataset, target_var):\n",
    "    pred = dataset.copy()\n",
    "    model_list = list(pred.columns.values)\n",
    "    y = pred[[target_var]]\n",
    "    model_list.remove(target_var)\n",
    "\n",
    "    diebold_mariano_dmstat_df = pd.DataFrame(index=model_list, columns=model_list)\n",
    "    diebold_mariano_pvalue_df = pd.DataFrame(index=model_list, columns=model_list)\n",
    "    \n",
    "    for model_1 in model_list:\n",
    "        for model_2 in model_list:\n",
    "            if model_1 == model_2:\n",
    "                dm_stat, pvalue = 0, 0\n",
    "            else:\n",
    "                if pd.isna(diebold_mariano_pvalue_df.loc[model_2, model_1]):\n",
    "                    model_1_val, model_2_val, y_val = prepare_diebold_mariano(pred, target_var, model_1, model_2)\n",
    "                    dm_stat, pvalue = dm_test(y_val, model_1_val, model_2_val, one_sided=True)\n",
    "                    pvalue = evaluate_pvalue(pvalue)\n",
    "                else:\n",
    "                    dm_stat, pvalue = 0, 0\n",
    "            diebold_mariano_dmstat_df.at[model_1, model_2], diebold_mariano_pvalue_df.at[model_1, model_2] = dm_stat, pvalue\n",
    "    return diebold_mariano_dmstat_df, diebold_mariano_pvalue_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
